{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coranvirus_Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Coronavirus (COVID-19) Evolution and Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Import, clean and format confirmed Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Sources\n",
    "\n",
    "#### Source of data: https://raw.githubusercontent.com/CSSEGISandData, this is updated by John Hopkins Univiersity from a variety of government and NGO sources\n",
    "#### For full list of sources see: https://systems.jhu.edu/research/public-health/ncov/\n",
    "#### originally had been pulling directly from google sheets as per\n",
    "\n",
    "#### csv_url='http://spreadsheets.google.com/ccc?key=1UF2pSkFTURko2OvfHWWlFpDFAr1UxCBA4JLwlSP6KFo&output=csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as rs\n",
    "import pandas as pd\n",
    "\n",
    "csv_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Confirmed.csv'\n",
    "\n",
    "res=rs.get(url=csv_url)\n",
    "open('coronavirus_stats_conf2.csv', 'wb').write(res.content)\n",
    "\n",
    "read_conf = pd.read_csv('/Users/neil.watt/Documents/PythonScripts/Coronavirus/coronavirus_stats_conf2.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "#read_conf = pd.read_csv('/Users/neilwatt/Documents/Projects/Coronavirus/Coronavirus_Analysis/coronavirus_stats_conf2.csv', encoding = \"ISO-8859-1\")\n",
    "#replace blank province/state with nil\n",
    "read_conf['Province/State'].fillna('', inplace=True)\n",
    "\n",
    "#replace all other NaNs with 0\n",
    "read_conf.fillna(0, inplace=True)\n",
    "\n",
    "#create new name key column\n",
    "read_conf.insert(1, 'name', read_conf[\"Province/State\"].map(str) +' '+ read_conf[\"Country/Region\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Formatting and Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data in the CSV file needs to be re-worked and reshaped to facilitate convenient plotting.\n",
    "#### 1.  I first sliced the csv into two dataframes and then transposed the date columns (from column 5 onwards)\n",
    "#### 2. Next step was to rework the cases figures into a \"Cases\" column according to the Date column (which is created from the index date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#slice confirmed cases into two dfs in order to recombine transpose of the cases table with the regions\n",
    "\n",
    "read_conf_copy=read_conf.copy()\n",
    "\n",
    "#this creates a dataframe which is a slice with just the city rows (which will be replciated for each date)\n",
    "\n",
    "new_city=read_conf_copy.iloc[:,0:5]\n",
    "read_conf_copy=read_conf.copy()\n",
    "\n",
    "#this slices the date columns from the original\n",
    "\n",
    "#new_df2=read_conf_copy.iloc[:,6:]\n",
    "new_df2=read_conf_copy.iloc[:,5:]\n",
    "\n",
    "\n",
    "\n",
    "#transpose date and cases columnes\n",
    "new_df2_T=new_df2.T\n",
    "\n",
    "\n",
    "#number cities\n",
    "i_points=len(read_conf.index)\n",
    "\n",
    "#new_df2_T_1=new_df2_T.iloc[:,0:1]\n",
    "new_df2_T['Date'] = new_df2_T.index\n",
    "\n",
    "d = {}\n",
    "for i in range (0,i_points):\n",
    "    d[i]=new_df2_T.iloc[:,i].to_frame()\n",
    "    d[i].rename(columns={ d[i].columns[0]: \"Cases\" }, inplace = True)\n",
    "    d[i]['Date'] = d[i].index\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframes are subsequently re-combined and duplicates are eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#recombine dataframes\n",
    "\n",
    "dfs={}\n",
    "for i in range (0,i_points):\n",
    "\n",
    "\n",
    "#match with df1 based on column header\n",
    "    city_row=new_city.loc[i,:].to_frame()\n",
    "    city_row_T=city_row.T\n",
    "\n",
    "#needs to be repeated same number of times as date columns\n",
    "    n_times=len(d[i].index)\n",
    "    city_row_T2=pd.concat([city_row_T]*n_times)\n",
    "\n",
    "\n",
    "    d[i]['tmp'] = 1\n",
    "    city_row_T2['tmp'] = 1\n",
    "\n",
    "    dfs[i] = pd.merge(city_row_T2,d[i], on=['tmp'])\n",
    "    dfs[i] = dfs[i].drop('tmp', axis=1)\n",
    "    dfs[i]= dfs[i].drop_duplicates(subset='Date')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#append dataframes into master format\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "#in order to model number of days from onset, convert to datetime\n",
    "final_df['Date_proper'] = pd.to_datetime(final_df['Date'])\n",
    "\n",
    "#Create new days column based on number of days from first row\n",
    "final_df['Day']=((final_df['Date_proper']-final_df['Date_proper'].iloc[0]).dt.total_seconds())/(24*60*60)+1\n",
    "\n",
    "\n",
    "final_df.to_csv(r'Coronavirus_cases_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#exlcude Mainland China cases for international cases\n",
    "final_df_international=final_df[final_df['Country/Region']!='Mainland China']\n",
    "\n",
    "\n",
    "#mainland china df\n",
    "final_df_MChina=final_df[final_df['Country/Region']=='Mainland China']\n",
    "\n",
    "#mainland exlcuding Hubei\n",
    "final_df_MChina_exHubei=final_df_MChina[final_df_MChina['Province/State']!='Hubei']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import, clean and format deaths and recovereries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dead \n",
    "\n",
    "dead_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Deaths.csv'\n",
    "dead_res=rs.get(url=dead_url)\n",
    "open('coronavirus_stats_dead.csv', 'wb').write(dead_res.content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#read_dead = pd.read_csv('/Users/neilwatt/Documents/Projects/Coronavirus/Coronavirus_Analysis/coronavirus_stats_dead.csv', encoding = \"ISO-8859-1\")\n",
    "read_dead = pd.read_csv('/Users/neil.watt/Documents/PythonScripts/Coronavirus/coronavirus_stats_dead.csv', encoding = \"ISO-8859-1\")\n",
    "#replace blank province/state with nil\n",
    "read_dead['Province/State'].fillna('', inplace=True)\n",
    "\n",
    "#replace all other NaNs with 0\n",
    "read_dead.fillna(0, inplace=True)\n",
    "\n",
    "#create new name key column\n",
    "read_dead.insert(1, 'name', read_dead[\"Province/State\"].map(str) +' '+ read_dead[\"Country/Region\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import recovered\n",
    "\n",
    "rec_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Recovered.csv'\n",
    "rec_res=rs.get(url=rec_url)\n",
    "open('coronavirus_stats_rec.csv', 'wb').write(rec_res.content)\n",
    "\n",
    "\n",
    "#read_rec = pd.read_csv('/Users/neilwatt/Documents/Projects/Coronavirus/Coronavirus_Analysis/coronavirus_stats_rec.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "read_rec = pd.read_csv('/Users/neil.watt/Documents/PythonScripts/Coronavirus/coronavirus_stats_rec.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "#replace blank province/state with nil\n",
    "read_rec['Province/State'].fillna('', inplace=True)\n",
    "\n",
    "#replace all other NaNs with 0\n",
    "read_rec.fillna(0, inplace=True)\n",
    "\n",
    "#create new name key column\n",
    "read_rec.insert(1, 'name', read_rec[\"Province/State\"].map(str) +' '+ read_rec[\"Country/Region\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### format dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice dead cases into two dfs in order to recombine transpose of the cases table with the regions\n",
    "\n",
    "read_dead_copy=read_dead.copy()\n",
    "\n",
    "#this creayes a dataframe which is a slice with just the city rows (which will be replciated for each date)\n",
    "new_city_dead=read_dead_copy.iloc[:,0:5]\n",
    "read_dead_copy=read_dead.copy()\n",
    "\n",
    "#this slices the date columns from the original\n",
    "new_df2_dead=read_dead_copy.iloc[:,5:]\n",
    "\n",
    "#transpose date and cases columnes\n",
    "new_df2_T_dead=new_df2_dead.T\n",
    "\n",
    "\n",
    "#number cities\n",
    "i_points_dead=len(read_dead.index)\n",
    "\n",
    "#new_df2_T_1=new_df2_T.iloc[:,0:1]\n",
    "new_df2_T_dead['Date'] = new_df2_T_dead.index\n",
    "\n",
    "d_dead = {}\n",
    "for i in range (0,i_points):\n",
    "    d_dead[i]=new_df2_T_dead.iloc[:,i].to_frame()\n",
    "    d_dead[i].rename(columns={ d_dead[i].columns[0]: \"Cases\" }, inplace = True)\n",
    "    d_dead[i]['Date'] = d_dead[i].index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recombine dataframes (deaths)\n",
    "\n",
    "dfs_dead={}\n",
    "for i in range (0,i_points_dead):\n",
    "\n",
    "\n",
    "#match with df1 based on column header\n",
    "    city_row_dead=new_city_dead.loc[i,:].to_frame()\n",
    "    city_row_T_dead=city_row_dead.T\n",
    "\n",
    "#needs to be repeated same number of times as date columns\n",
    "    n_times_dead=len(d_dead[i].index)\n",
    "    city_row_T2_dead=pd.concat([city_row_T_dead]*n_times_dead)\n",
    "\n",
    "\n",
    "    d_dead[i]['tmp'] = 1\n",
    "    city_row_T2_dead['tmp'] = 1\n",
    "\n",
    "    dfs_dead[i] = pd.merge(city_row_T2_dead,d_dead[i], on=['tmp'])\n",
    "    dfs_dead[i] = dfs_dead[i].drop('tmp', axis=1)\n",
    "    dfs_dead[i]= dfs_dead[i].drop_duplicates(subset='Date')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append dataframes into master format\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "final_df_dead = pd.concat(dfs_dead, ignore_index=True)\n",
    "#in order to model number of days from onset, convert to datetime\n",
    "final_df_dead['Date_proper'] = pd.to_datetime(final_df_dead['Date'])\n",
    "\n",
    "#Create new days column based on number of days from first row\n",
    "final_df_dead['Day']=((final_df_dead['Date_proper']-final_df_dead['Date_proper'].iloc[0]).dt.total_seconds())/(24*60*60)+1\n",
    "\n",
    "\n",
    "final_df_dead.to_csv(r'Coronavirus_cases_dead_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exlcude Mainland China cases for international cases\n",
    "final_df_dead_international=final_df_dead[final_df_dead['Country/Region']!='Mainland China']\n",
    "\n",
    "\n",
    "#mainland china df\n",
    "final_df_dead_MChina=final_df_dead[final_df_dead['Country/Region']=='Mainland China']\n",
    "\n",
    "#mainland exlcuding Hubei\n",
    "final_df_dead_MChina_exHubei=final_df_dead_MChina[final_df_dead_MChina['Province/State']!='Hubei']\n",
    "\n",
    "#hubei\n",
    "final_df_dead_MChina_Hubei=final_df_dead_MChina[final_df_dead_MChina['Province/State']=='Hubei']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### format recoveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-955b394b5a17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0md_rec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_df2_T_rec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0md_rec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m \u001b[0md_rec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Cases\"\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0md_rec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_rec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mPY2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mrename\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3776\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'axis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3777\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mapper'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3778\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3780\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mSubstitution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_shared_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mrename\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m                 \u001b[0mlevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_level_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             result._data = result._data.rename_axis(f, axis=baxis, copy=copy,\n\u001b[1;32m--> 973\u001b[1;33m                                                     level=level)\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mrename_axis\u001b[1;34m(self, mapper, axis, copy, level)\u001b[0m\n\u001b[0;32m   3337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3338\u001b[0m         \"\"\"\n\u001b[1;32m-> 3339\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3340\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_transform_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3341\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep, mgr)\u001b[0m\n\u001b[0;32m   3918\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3919\u001b[0m         return self.apply('copy', axes=new_axes, deep=deep,\n\u001b[1;32m-> 3920\u001b[1;33m                           do_integrity_check=False)\n\u001b[0m\u001b[0;32m   3921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3922\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m   3585\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3586\u001b[0m         bm = self.__class__(result_blocks, axes or self.axes,\n\u001b[1;32m-> 3587\u001b[1;33m                             do_integrity_check=do_integrity_check)\n\u001b[0m\u001b[0;32m   3588\u001b[0m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[0;32m   3284\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3286\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rebuild_blknos_and_blklocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_rebuild_blknos_and_blklocs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3373\u001b[0m             \u001b[0mrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3374\u001b[0m             \u001b[0mnew_blknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblkno\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3375\u001b[1;33m             \u001b[0mnew_blklocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew_blknos\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#slice dead cases into two dfs in order to recombine transpose of the cases table with the regions\n",
    "\n",
    "read_rec_copy=read_rec.copy()\n",
    "\n",
    "#this creayes a dataframe which is a slice with just the city rows (which will be replciated for each date)\n",
    "new_city_rec=read_rec_copy.iloc[:,0:5]\n",
    "read_rec_copy=read_rec.copy()\n",
    "\n",
    "#this slices the date columns from the original\n",
    "new_df2_rec=read_rec_copy.iloc[:,5:]\n",
    "\n",
    "#transpose date and cases columnes\n",
    "new_df2_T_rec=new_df2_rec.T\n",
    "\n",
    "\n",
    "#number cities\n",
    "i_points_rec=len(read_rec.index)\n",
    "\n",
    "#new_df2_T_1=new_df2_T.iloc[:,0:1]\n",
    "new_df2_T_rec['Date'] = new_df2_T_rec.index\n",
    "\n",
    "d_rec = {}\n",
    "for i in range (0,i_points):\n",
    "    d_rec[i]=new_df2_T_rec.iloc[:,i].to_frame()\n",
    "    d_rec[i].rename(columns={ d_rec[i].columns[0]: \"Cases\" }, inplace = True)\n",
    "    d_rec[i]['Date'] = d_rec[i].index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recombine dataframes (deaths)\n",
    "\n",
    "dfs_rec={}\n",
    "for i in range (0,i_points_rec):\n",
    "\n",
    "\n",
    "#match with df1 based on column header\n",
    "    city_row_rec=new_city_rec.loc[i,:].to_frame()\n",
    "    city_row_T_rec=city_row_rec.T\n",
    "\n",
    "#needs to be repeated same number of times as date columns\n",
    "    n_times_rec=len(d_rec[i].index)\n",
    "    city_row_T2_rec=pd.concat([city_row_T_rec]*n_times_rec)\n",
    "\n",
    "\n",
    "    d_rec[i]['tmp'] = 1\n",
    "    city_row_T2_rec['tmp'] = 1\n",
    "\n",
    "    dfs_rec[i] = pd.merge(city_row_T2_rec,d_rec[i], on=['tmp'])\n",
    "    dfs_rec[i] = dfs_rec[i].drop('tmp', axis=1)\n",
    "    dfs_rec[i]= dfs_rec[i].drop_duplicates(subset='Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append dataframes into master format\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "final_df_rec = pd.concat(dfs_rec, ignore_index=True)\n",
    "#in order to model number of days from onset, convert to datetime\n",
    "final_df_rec['Date_proper'] = pd.to_datetime(final_df_rec['Date'])\n",
    "\n",
    "#Create new days column based on number of days from first row\n",
    "final_df_rec['Day']=((final_df_rec['Date_proper']-final_df_rec['Date_proper'].iloc[0]).dt.total_seconds())/(24*60*60)+1\n",
    "\n",
    "\n",
    "final_df_rec.to_csv(r'Coronavirus_cases_rec_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exlcude Mainland China cases for international cases\n",
    "final_df_rec_international=final_df_rec[final_df_rec['Country/Region']!='Mainland China']\n",
    "\n",
    "\n",
    "#mainland china df\n",
    "final_df_rec_MChina=final_df_rec[final_df_rec['Country/Region']=='Mainland China']\n",
    "\n",
    "#mainland exlcuding Hubei\n",
    "final_df_rec_MChina_exHubei=final_df_rec_MChina[final_df_rec_MChina['Province/State']!='Hubei']\n",
    "\n",
    "#hubei\n",
    "final_df_rec_MChina_Hubei=final_df_rec_MChina[final_df_rec_MChina['Province/State']=='Hubei']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### adding top regions by number of cases for further analysis\n",
    "\n",
    "\n",
    "#hubei\n",
    "final_df_MChina_Hubei=final_df_MChina[final_df_MChina['Province/State']=='Hubei']\n",
    "\n",
    "#Guangdong\n",
    "final_df_MChina_Guangdong=final_df_MChina[final_df_MChina['Province/State']=='Guangdong']\n",
    "#Zhejiang\n",
    "final_df_MChina_Zhejiang=final_df_MChina[final_df_MChina['Province/State']=='Zhejiang']\n",
    "#Henan\n",
    "final_df_MChina_Henan=final_df_MChina[final_df_MChina['Province/State']=='Henan']\n",
    "\n",
    "#Hunan\n",
    "final_df_MChina_Hunan=final_df_MChina[final_df_MChina['Province/State']=='Hunan']\n",
    "\n",
    "#Anhui\n",
    "final_df_MChina_Anhui=final_df_MChina[final_df_MChina['Province/State']=='Anhui']\n",
    "\n",
    "\n",
    "#Jiangxi\n",
    "final_df_MChina_Jiangxi=final_df_MChina[final_df_MChina['Province/State']=='Jiangxi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Deaths and Recoveries vs Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_dead_MChina_Hubei['% Dead']=final_df_dead_MChina_Hubei['Cases']/final_df_MChina_Hubei['Cases']*100\n",
    "\n",
    "final_df_rec_MChina_Hubei['% Rec']=final_df_rec_MChina_Hubei['Cases']/final_df_MChina_Hubei['Cases']*100\n",
    "\n",
    "\n",
    "#Hubei Deaths vs cases\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=final_df_dead_MChina_Hubei['Date'], y=final_df_dead_MChina_Hubei['% Dead'], name=\"% Hubei Deaths\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=final_df_MChina_Hubei['Date'], y=final_df_rec_MChina_Hubei['% Rec'], name=\"% Hubei Recoveries\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "#fig.add_trace(\n",
    " #   go.Scatter(x=final_df_MChina_Hubei['Date'], y=final_df_MChina_Hubei['Cases'], name=\"Hubei Cases\"),\n",
    "  #  secondary_y=True,\n",
    "#)\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Hubei % Recoveries and Deaths\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"% Deaths and Recoveries\", secondary_y=False)\n",
    "#fig.update_yaxes(title_text=\"Cases\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hubei Deaths vs cases\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=final_df_dead_MChina_Hubei['Date'], y=final_df_dead_MChina_Hubei['Cases'], name=\"Hubei Deaths\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=final_df_MChina_Hubei['Date'], y=final_df_rec_MChina_Hubei['Cases'], name=\"Hubei Recoveries\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=final_df_MChina_Hubei['Date'], y=final_df_MChina_Hubei['Cases'], name=\"Hubei Cases\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Hubei Coronavirus Cases vs Recoveries and Deaths\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"Deaths and Recoveries\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Cases\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hubei Deaths vs cases\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=final_df_dead_MChina_Hubei['Date'], y=final_df_dead_MChina_Hubei['Cases'], name=\"Hubei Deaths\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=final_df_MChina_Hubei['Date'], y=final_df_rec_MChina_Hubei['Cases'], name=\"Hubei Recoveries\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=final_df_MChina_Hubei['Date'], y=final_df_MChina_Hubei['Cases'], name=\"Hubei Cases\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Hubei Coronavirus Cases vs Recoveries and Deaths\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"Deaths and Recoveries\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Cases\", secondary_y=True)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mainland excluding Hubei\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.line(final_df_dead_MChina_exHubei,title=\"Mainland China Reported Coronavirus Deaths (excl Hubei)\", x=\"Date\", y=\"Cases\", color=\"name\", line_group=\"name\", hover_name=\"name\",render_mode=\"svg\")\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace(\"name=\", \"\")))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Cases by Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  plotly.express as px\n",
    "#https://plot.ly/python-api-reference/generated/plotly.express.scatter_geo.html\n",
    "scale=0.1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = px.scatter_geo(final_df_MChina , lat=\"Lat\",lon=\"Long\",\n",
    "                    color=\"Cases\",\n",
    "                    #size=\"Cases\",\n",
    "                     size_max=70,\n",
    "                    size='Cases',\n",
    "                     #size=\"Cases\",\n",
    "                     title=\"Coronavirus Cases by Day (Mainland China))\",\n",
    "                     \n",
    "                   hover_name=\"name\", \n",
    "                     #labels=\"name\",\n",
    "                    color_continuous_scale=px.colors.sequential.RdBu[::-1],\n",
    "                     # color_continuous_scale=px.colors.sequential.Plasma,\n",
    "                     #plotly.express.colors.diverging\n",
    "                    animation_frame=\"Date\"\n",
    "                    \n",
    "                    \n",
    "                    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non Mainland China only\n",
    "\n",
    "import  plotly.express as px\n",
    "#https://plot.ly/python-api-reference/generated/plotly.express.scatter_geo.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = px.scatter_geo(final_df_international , lat=\"Lat\",lon=\"Long\",\n",
    "                    color=\"Cases\",\n",
    "                    #size=\"Cases\",\n",
    "                     size_max=40,\n",
    "                    size='Cases',\n",
    "                     #size=\"Cases\",\n",
    "                     title=\"Coronavirus Cases by Day (exl Mainland CN))\",\n",
    "                     \n",
    "                   hover_name=\"name\", \n",
    "                     #labels=\"name\",\n",
    "                  #  color_continuous_scale=px.colors.sequential.RdBu[::-1],\n",
    "                    # color_continuous_scale=px.colors.sequential.RdBu[::-1],\n",
    "                     # color_continuous_scale=px.colors.sequential.Plasma,\n",
    "                     #plotly.express.colors.diverging\n",
    "                    animation_frame=\"Date\"\n",
    "                   \n",
    "                    \n",
    "                    \n",
    "                    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot of international cases\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.line(final_df_international,title=\"International Cases\", x=\"Date\", y=\"Cases\", color=\"name\", line_group=\"name\", hover_name=\"name\",render_mode=\"svg\")\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace(\"name=\", \"\")))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_international"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_italy=final_df_international[final_df_international['Country/Region']=='Italy']\n",
    "                                \n",
    "df_italy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top=final_df_international[final_df_international['Country/Region'].isin(['South Korea','Italy','Iran','Japan','France','Germany','Spain'\n",
    ",'Singapore'\n",
    ",'Hong Kong'\n",
    ",'Kuwait'\n",
    ",'Switzerland'\n",
    ",'UK'\n",
    "])]\n",
    "df_top.head()\n",
    "\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# plot of international cases- \n",
    "\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.line(df_top,title=\"International Cases (top 12)\", x=\"Date\", y=\"Cases\", color=\"name\", line_group=\"name\", hover_name=\"name\",render_mode=\"svg\")\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace(\"name=\", \"\")))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.line(final_df_MChina_exHubei,title=\"Mainland China Reported Coronavirus Cases (excl Hubei)\", x=\"Date\", y=\"Cases\", color=\"name\", line_group=\"name\", hover_name=\"name\",render_mode=\"svg\")\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace(\"name=\", \"\")))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig = px.line(final_df_MChina_Hubei,title=\"Hubei Coronavirus Cases\", x=\"Date\", y=\"Cases\", color=\"name\", line_group=\"name\", hover_name=\"name\",render_mode=\"svg\")\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace(\"name=\", \"\")))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define exponential function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(b * x) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf range https://lmfit.github.io/lmfit-py/model.html\n",
    "\n",
    "Cases=final_df_MChina_Hubei['Cases']\n",
    "Days=final_df_MChina_Hubei['Day']\n",
    "\n",
    "#empirical data\n",
    "xdata = Days\n",
    "y = Cases\n",
    "ydata = y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "popt, pcov = curve_fit(func, xdata, ydata)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(xdata, func(xdata, *popt), 'r-',label='Exp Model: a=%5.3f, b=%5.3f' % tuple(popt))\n",
    "ax.plot(xdata, ydata, 'b-', label='Reported Cases')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Cases')\n",
    "ax.set_title('Hubei Coronavirus Cases by Day')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_MChina_Hubei.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Future Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "final_df_MChina_Hubei['y']=final_df_MChina_Hubei['Cases']\n",
    "final_df_MChina_Hubei['ds']=final_df_MChina_Hubei['Date']\n",
    "\n",
    "\n",
    "#Guangdong\n",
    "final_df_MChina_Guangdong['y']=final_df_MChina_Guangdong['Cases']\n",
    "final_df_MChina_Guangdong['ds']=final_df_MChina_Guangdong['Date']\n",
    "\n",
    "#Zhejiang\n",
    "final_df_MChina_Zhejiang['y']=final_df_MChina_Zhejiang['Cases']\n",
    "final_df_MChina_Zhejiang['ds']=final_df_MChina_Zhejiang['Date']\n",
    "\n",
    "#Henan\n",
    "final_df_MChina_Henan['y']=final_df_MChina_Henan['Cases']\n",
    "final_df_MChina_Henan['ds']=final_df_MChina_Henan['Date']\n",
    "\n",
    "#Hunan\n",
    "final_df_MChina_Hunan['y']=final_df_MChina_Hunan['Cases']\n",
    "final_df_MChina_Hunan['ds']=final_df_MChina_Hunan['Date']\n",
    "\n",
    "#Anhui\n",
    "final_df_MChina_Anhui['y']=final_df_MChina_Anhui['Cases']\n",
    "final_df_MChina_Anhui['ds']=final_df_MChina_Anhui['Date']\n",
    "\n",
    "#Jiangxi\n",
    "final_df_MChina_Jiangxi['y']=final_df_MChina_Jiangxi['Cases']\n",
    "final_df_MChina_Jiangxi['ds']=final_df_MChina_Jiangxi['Date']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "\n",
    "\n",
    "#NOTE, DEFAULT changepoint_prior_scale IS 0.05, DECREASING makes less flexible, INCREASIGN makes more flexible\n",
    "changepoint_prior_scale_new=0.065\n",
    "periods_new=30\n",
    "\n",
    "\n",
    "df_Hubei=final_df_MChina_Hubei\n",
    "gm_prophet = Prophet(interval_width=0.95, changepoint_prior_scale=changepoint_prior_scale_new)\n",
    "gm_prophet.fit(df_Hubei)\n",
    "future = gm_prophet.make_future_dataframe(periods=periods_new)\n",
    "\n",
    "\n",
    "\n",
    "#Guangdong\n",
    "\n",
    "df_Guangdong=final_df_MChina_Guangdong\n",
    "gm_prophet_Guangdong = Prophet(interval_width=0.95, changepoint_prior_scale=changepoint_prior_scale_new)\n",
    "gm_prophet_Guangdong.fit(df_Guangdong)\n",
    "future_Guangdong = gm_prophet_Guangdong.make_future_dataframe(periods=periods_new)\n",
    "\n",
    "\n",
    "#Zhejiang\n",
    "df_Zhejiang=final_df_MChina_Zhejiang\n",
    "gm_prophet_Zhejiang = Prophet(interval_width=0.95, changepoint_prior_scale=changepoint_prior_scale_new)\n",
    "gm_prophet_Zhejiang.fit(df_Zhejiang)\n",
    "future_Zhejiang = gm_prophet_Zhejiang.make_future_dataframe(periods=periods_new)\n",
    "\n",
    "\n",
    "#Henan\n",
    "df_Henan=final_df_MChina_Henan\n",
    "gm_prophet_Henan = Prophet(interval_width=0.95, changepoint_prior_scale=changepoint_prior_scale_new)\n",
    "gm_prophet_Henan.fit(df_Henan)\n",
    "future_Henan = gm_prophet_Henan.make_future_dataframe(periods=periods_new)\n",
    "\n",
    "\n",
    "\n",
    "#Hunan\n",
    "df_Hunan=final_df_MChina_Hunan\n",
    "gm_prophet_Hunan = Prophet(interval_width=0.95, changepoint_prior_scale=changepoint_prior_scale_new)\n",
    "gm_prophet_Hunan.fit(df_Hunan)\n",
    "future_Hunan = gm_prophet_Hunan.make_future_dataframe(periods=periods_new)\n",
    "\n",
    "\n",
    "#Anhui\n",
    "df_Anhui=final_df_MChina_Anhui\n",
    "gm_prophet_Anhui = Prophet(interval_width=0.95, changepoint_prior_scale=changepoint_prior_scale_new)\n",
    "gm_prophet_Anhui.fit(df_Anhui)\n",
    "future_Anhui = gm_prophet_Anhui.make_future_dataframe(periods=periods_new)\n",
    "\n",
    "\n",
    "#Jiangxi\n",
    "df_Jiangxi=final_df_MChina_Jiangxi\n",
    "gm_prophet_Jiangxi = Prophet(interval_width=0.95, changepoint_prior_scale=changepoint_prior_scale_new)\n",
    "gm_prophet_Jiangxi.fit(df_Jiangxi)\n",
    "future_Jiangxi = gm_prophet_Jiangxi.make_future_dataframe(periods=periods_new)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create forecast with upper and lower bounds\n",
    "forecast = gm_prophet.predict(future)\n",
    "#forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "\n",
    "#Guangdong\n",
    "forecast_Guangdong = gm_prophet_Guangdong.predict(future_Guangdong)\n",
    "\n",
    "\n",
    "#Zhejiang\n",
    "forecast_Zhejiang = gm_prophet_Zhejiang.predict(future_Zhejiang)\n",
    "\n",
    "#Henan\n",
    "\n",
    "forecast_Henan = gm_prophet_Henan.predict(future_Henan)\n",
    "\n",
    "#Hunan\n",
    "forecast_Hunan = gm_prophet_Hunan.predict(future_Hunan)\n",
    "\n",
    "\n",
    "#Anhui\n",
    "forecast_Anhui = gm_prophet_Anhui.predict(future_Anhui)\n",
    "\n",
    "#Jiangxi\n",
    "forecast_Jiangxi = gm_prophet_Jiangxi.predict(future_Jiangxi)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig_Guangdong = gm_prophet_Guangdong.plot(forecast_Guangdong)\n",
    "changepoints_Guangdong = add_changepoints_to_plot(fig_Guangdong.gca(), gm_prophet_Guangdong, forecast_Guangdong)\n",
    "\n",
    "\n",
    "fig_Zhejiang = gm_prophet_Zhejiang.plot(forecast_Zhejiang)\n",
    "changepoints_Zhejiang = add_changepoints_to_plot(fig_Zhejiang.gca(), gm_prophet_Zhejiang, forecast_Zhejiang)\n",
    "\n",
    "fig_Henan = gm_prophet_Henan.plot(forecast_Henan)\n",
    "changepoints_Henan = add_changepoints_to_plot(fig_Henan.gca(), gm_prophet_Henan, forecast_Henan)\n",
    "\n",
    "\n",
    "fig_Hunan = gm_prophet_Hunan.plot(forecast_Hunan)\n",
    "changepoints_Hunan = add_changepoints_to_plot(fig_Hunan.gca(), gm_prophet_Hunan, forecast_Hunan)\n",
    "\n",
    "\n",
    "fig_Anhui = gm_prophet_Anhui.plot(forecast_Anhui)\n",
    "changepoints_Anhui = add_changepoints_to_plot(fig_Anhui.gca(), gm_prophet_Anhui, forecast_Anhui)\n",
    "\n",
    "\n",
    "fig_Jiangxi = gm_prophet_Jiangxi.plot(forecast_Jiangxi)\n",
    "changepoints_Jiangxi = add_changepoints_to_plot(fig_Jiangxi.gca(), gm_prophet_Jiangxi, forecast_Jiangxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing trajectory of new cases for various provinces (Mainland China)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot major provinces showing similar change in trend (after redfinition of \"infected\")\n",
    "#https://www.taiwannews.com.tw/en/news/3874490\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(4, 1, 1)\n",
    "gm_prophet_Anhui.plot(forecast_Anhui, ax=ax1)\n",
    "ax1.set_title('Anhui, Confirmed Coronavirus Cases')\n",
    "changepoints = add_changepoints_to_plot(fig.gca(), gm_prophet_Anhui, forecast_Anhui)\n",
    "\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(4, 1, 2)\n",
    "gm_prophet_Guangdong.plot(forecast_Guangdong, ax=ax2)\n",
    "changepoints_Guangdong = add_changepoints_to_plot(fig.gca(), gm_prophet_Guangdong, forecast_Guangdong)\n",
    "ax2.set_title('Guangdong, Confirmed Coronavirus Cases')\n",
    "\n",
    "ax3 = fig.add_subplot(4, 1, 3)\n",
    "changepoints_Henan = add_changepoints_to_plot(fig.gca(), gm_prophet_Henan, forecast_Henan)\n",
    "gm_prophet_Henan.plot(forecast_Henan, ax=ax3)\n",
    "ax3.set_title('Henan, Confirmed Coronavirus Cases')\n",
    "\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(4, 1, 4)\n",
    "changepoints_Hunan = add_changepoints_to_plot(fig.gca(), gm_prophet_Hunan, forecast_Hunan)\n",
    "gm_prophet_Hunan.plot(forecast_Hunan, ax=ax4)\n",
    "ax4.set_title('Hunan, Confirmed Coronavirus Cases')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "axes = fig.get_axes()\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_ylabel('Cases')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].set_ylabel('Cases')\n",
    "axes[2].set_xlabel('')\n",
    "axes[2].set_ylabel('Cases')\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].set_ylabel('Cases')\n",
    "\n",
    "fig.savefig('temp.png', dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = gm_prophet.plot(forecast)\n",
    "plt.title('Hubei Coronavirus Cases by Day, actual and forecasted');\n",
    "axes = fig1.get_axes()\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Cases')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added change points to forecast\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "fig2= gm_prophet.plot(forecast)\n",
    "a = add_changepoints_to_plot(fig2.gca(), gm_prophet, forecast)\n",
    "plt.title('Hubei Coronavirus Cases by Day, actual and forecasted with changepoints');\n",
    "axes = fig2.get_axes()\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Cases')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flights Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pcfg.eu/posts/how-to-plot-flight-routes-using-plotly/\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "#import plotly.plotly as py\n",
    "import plotly.offline as ol\n",
    "from geographiclib.geodesic import Geodesic\n",
    "geod = Geodesic.WGS84\n",
    "\n",
    "# Define function to calculate distance (in meters) between two points\n",
    "def dist(p1Lat, p1Lon, p2Lat, p2Lon):\n",
    "    return geod.Inverse(p1Lat, p1Lon, p2Lat, p2Lon, Geodesic.DISTANCE)['s12']\n",
    "\n",
    "# Read the data into a dataframe (specifying the column names)\n",
    "\n",
    "#read_conf = pd.read_csv('/Users/neil.watt/Documents/PythonScripts/Coronavirus/coronavirus_stats_conf2.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "\n",
    "#df = pd.read_csv('routes.dat', sep=',', header=None,\n",
    " #                names=['Airline','Airline ID','Source airport','Source airport ID',\n",
    "  #                      'Destination airport','Destination airport ID','Codeshare',\n",
    "   #                     'Stops','Equipment'])\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/neil.watt/Documents/PythonScripts/Coronavirus/routes.dat.txt',sep=',', header=None,\n",
    "                 names=['Airline','Airline ID','Source airport','Source airport ID',\n",
    "                        'Destination airport','Destination airport ID','Codeshare',\n",
    "                        'Stops','Equipment'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates (only one trajectory per route)\n",
    "df = df[['Source airport','Destination airport']].drop_duplicates(keep='first', inplace=False)\n",
    "\n",
    "# Read the data into a dataframe (specifying the column names)\n",
    "df_airports = pd.read_csv('/Users/neil.watt/Documents/PythonScripts/Coronavirus/airports.dat.txt', sep=',', header=None,\n",
    "                          names=['Airport','Name','City','Country','IATA','ICAO',\n",
    "                                 'Latitude','Longitude','Altitude','Timezone','DST',\n",
    "                                 'Tz','Type','Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL LONDON AIRPORTS\n",
    "\n",
    "# Select only those routes starting or ending from a London Airport\n",
    "# in order London City, Heathrow, Gatwick, Luton, Stansted, Southend\n",
    "df = df.loc[(df['Source airport'].isin(['LCY','LHR','LGW','LTN','STN','SEN'])) | (df['Destination airport'].isin(['LCY','LHR','LGW','LTN','STN','SEN']))]\n",
    "\n",
    "# Append the origin airport's coordinates to the routes' dataframe\n",
    "df = pd.merge(df, df_airports[['IATA','Latitude','Longitude']],\n",
    "              how='inner', left_on='Source airport', right_on='IATA', suffixes=('_Orig','_Dest'))\n",
    "\n",
    "# Append the destination airport's coordinates to the routes' dataframe\n",
    "df = pd.merge(df, df_airports[['IATA','Latitude','Longitude']],\n",
    "              how='inner', left_on='Destination airport', right_on='IATA', suffixes=('_Orig','_Dest'))\n",
    "\n",
    "# Keep only Origin/Destination IATA ID columns, and their Latitude/Longitude\n",
    "df = df.drop(columns=['Source airport','Destination airport'])\n",
    "\n",
    "# Calculate the distance (great circle distance) between the origin and destination airports\n",
    "df['Distance'] = ''\n",
    "for index, row in df.iterrows():\n",
    "    df.loc[df.index==index,'Distance'] = dist(row.Latitude_Orig, row.Longitude_Orig, row.Latitude_Dest, row.Longitude_Dest)/1000\n",
    "\n",
    "# Initialise the data list that will be used to feed the plot\n",
    "data = []\n",
    "\n",
    "# Append all airports (blue dots) to the map\n",
    "data.append(dict(\n",
    "                type = 'scattergeo',\n",
    "                locationmode = 'ISO-3',\n",
    "                showlegend = False,\n",
    "                lon = df_airports['Longitude'],\n",
    "                lat = df_airports['Latitude'],\n",
    "                hoverinfo = 'text',\n",
    "                text = df_airports['IATA'],\n",
    "                mode = 'markers',\n",
    "                marker = dict(\n",
    "                    size=2,\n",
    "                    color='rgb(0, 0, 255)',\n",
    "                    line = dict(\n",
    "                        width=3,\n",
    "                        color='rgba(68, 68, 68, 0)'\n",
    "                    )\n",
    "                ))\n",
    "        )\n",
    "\n",
    "# Append the longest route to the map\n",
    "data.append(\n",
    "        dict(\n",
    "            type = 'scattergeo',\n",
    "            locationmode = 'ISO-3',\n",
    "            name = 'Longest Route',\n",
    "            showlegend = True,\n",
    "            lon = [ df.loc[df['Distance']==df['Distance'].max(),'Longitude_Orig'].values[0], df.loc[df['Distance']==df['Distance'].max(),'Longitude_Dest'].values[0] ],\n",
    "            lat = [ df.loc[df['Distance']==df['Distance'].max(),'Latitude_Orig'].values[0], df.loc[df['Distance']==df['Distance'].max(),'Latitude_Dest'].values[0] ],\n",
    "            mode = 'lines',\n",
    "            line = dict(\n",
    "                width = 2,\n",
    "                color = 'red',\n",
    "            ),\n",
    "            opacity = 1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Append all other routes to the map\n",
    "for i in range(len(df)):\n",
    "    data.append(\n",
    "        dict(\n",
    "            type = 'scattergeo',\n",
    "            locationmode = 'ISO-3',\n",
    "            name = str(df['IATA_Orig'][i]) + ' - ' + str(df['IATA_Dest'][i]),\n",
    "            showlegend = True if df['IATA_Orig'][i] in ['LCY','LHR','LGW','LTN','STN','SEN'] else False,\n",
    "            lon = [ df['Longitude_Orig'][i], df['Longitude_Dest'][i] ],\n",
    "            lat = [ df['Latitude_Orig'][i], df['Latitude_Dest'][i] ],\n",
    "            mode = 'lines',\n",
    "            line = dict(\n",
    "                width = 1,\n",
    "                color = 'green',\n",
    "            ),\n",
    "            opacity = 0.3,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Define the plot's layout\n",
    "layout = dict(\n",
    "        title = 'Airports and Routes',\n",
    "        showlegend = True,\n",
    "        geo = dict(\n",
    "            scope='world',\n",
    "            projection=dict( type='azimuthal equal area' ),\n",
    "            showland = True,\n",
    "            landcolor = 'rgb(255, 255, 255)',\n",
    "            countrycolor = 'rgb(0, 0, 0)',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Create the figure to be plotted\n",
    "fig = dict( data=data, layout=layout )\n",
    "#py.plot(fig, world_readable=True)\n",
    "ol.plot(fig, filename='Airports and routes.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Indian Airports\n",
    "\n",
    "\n",
    "df_India=df_airports[df_airports['Country']=='India']\n",
    "India_list=df_India['IATA'].tolist() \n",
    "India_list_new=list(set(India_list))\n",
    "\n",
    "\n",
    "df_Iran=df_airports[df_airports['Country']=='Iran']\n",
    "Iran_list=df_Iran['IATA'].tolist() \n",
    "Iran_list_new=list(set(Iran_list))\n",
    "\n",
    "df_China=df_airports[df_airports['Country']=='China']\n",
    "China_list=df_China['IATA'].tolist() \n",
    "China_list_new=list(set(China_list))\n",
    "\n",
    "df_Italy=df_airports[df_airports['Country']=='Italy']\n",
    "Italy_list=df_Italy['IATA'].tolist() \n",
    "Italy_list_new=list(set(Italy_list))\n",
    "\n",
    "df_UK=df_airports[df_airports['Country']=='United Kingdom']\n",
    "UK_list=df_UK['IATA'].tolist() \n",
    "UK_list_new=list(set(UK_list))\n",
    "\n",
    "#final_df_MChina_Hubei=final_df_MChina[final_df_MChina['Province/State']=='Hubei']\n",
    "\n",
    "#inal_df_MChina_Hubei=final_df_MChina[final_df_MChina['Province/State']=='Hubei']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EOI',\n",
       " 'ILY',\n",
       " 'BZZ',\n",
       " 'NQT',\n",
       " 'PPW',\n",
       " 'BOH',\n",
       " 'LPL',\n",
       " 'OBN',\n",
       " 'ENK',\n",
       " 'PZE',\n",
       " 'LSI',\n",
       " 'GLA',\n",
       " 'VLY',\n",
       " 'WTN',\n",
       " 'SEN',\n",
       " 'CBG',\n",
       " 'OUK',\n",
       " 'SWS',\n",
       " 'FIE',\n",
       " 'ESH',\n",
       " 'HRT',\n",
       " 'BBP',\n",
       " 'QFO',\n",
       " 'QUY',\n",
       " 'HYC',\n",
       " 'EDI',\n",
       " 'QUG',\n",
       " 'GLO',\n",
       " 'NHT',\n",
       " 'BQH',\n",
       " 'LHR',\n",
       " '\\\\N',\n",
       " 'KOI',\n",
       " 'BHD',\n",
       " 'PLH',\n",
       " 'DND',\n",
       " 'SCS',\n",
       " 'HLE',\n",
       " 'LMO',\n",
       " 'LKZ',\n",
       " 'WIC',\n",
       " 'RCS',\n",
       " 'SQZ',\n",
       " 'UNT',\n",
       " 'KRH',\n",
       " 'HAW',\n",
       " 'MHZ',\n",
       " 'NCL',\n",
       " 'BRS',\n",
       " 'EXT',\n",
       " 'WRY',\n",
       " 'SOY',\n",
       " 'FSS',\n",
       " 'DSA',\n",
       " 'OXF',\n",
       " 'HUY',\n",
       " 'MAN',\n",
       " 'LYX',\n",
       " 'BEX',\n",
       " 'NDY',\n",
       " 'PSL',\n",
       " 'CVT',\n",
       " 'NQY',\n",
       " 'WRT',\n",
       " 'LBA',\n",
       " 'TRE',\n",
       " 'NWI',\n",
       " 'LWK',\n",
       " 'NRL',\n",
       " 'CSA',\n",
       " 'ISC',\n",
       " 'BLK',\n",
       " 'BHX',\n",
       " 'LGW',\n",
       " 'ADX',\n",
       " 'FAB',\n",
       " 'QLA',\n",
       " 'QCY',\n",
       " 'ODH',\n",
       " 'GBA',\n",
       " 'SOU',\n",
       " 'MSE',\n",
       " 'LEQ',\n",
       " 'YEO',\n",
       " 'FFD',\n",
       " 'CAX',\n",
       " 'EMA',\n",
       " 'BRR',\n",
       " 'LDY',\n",
       " 'LYM',\n",
       " 'CEG',\n",
       " 'CAL',\n",
       " 'BWF',\n",
       " 'LYE',\n",
       " 'STN',\n",
       " 'PIK',\n",
       " 'LCY',\n",
       " 'BBS',\n",
       " 'SYY',\n",
       " 'ABZ',\n",
       " 'MME',\n",
       " 'BEB',\n",
       " 'LTN',\n",
       " 'INV',\n",
       " 'BEQ',\n",
       " 'FZO',\n",
       " 'KNF',\n",
       " 'BFS',\n",
       " 'CWL']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UK_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['LCY','LHR','LGW','LTN','STN','SEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "#import plotly.plotly as py\n",
    "import plotly.offline as ol\n",
    "from geographiclib.geodesic import Geodesic\n",
    "geod = Geodesic.WGS84\n",
    "\n",
    "# Define function to calculate distance (in meters) between two points\n",
    "def dist(p1Lat, p1Lon, p2Lat, p2Lon):\n",
    "    return geod.Inverse(p1Lat, p1Lon, p2Lat, p2Lon, Geodesic.DISTANCE)['s12']\n",
    "\n",
    "# Read the data into a dataframe (specifying the column names)\n",
    "\n",
    "#read_conf = pd.read_csv('/Users/neil.watt/Documents/PythonScripts/Coronavirus/coronavirus_stats_conf2.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "\n",
    "#df = pd.read_csv('routes.dat', sep=',', header=None,\n",
    " #                names=['Airline','Airline ID','Source airport','Source airport ID',\n",
    "  #                      'Destination airport','Destination airport ID','Codeshare',\n",
    "   #                     'Stops','Equipment'])\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/neil.watt/Documents/PythonScripts/Coronavirus/routes.dat.txt',sep=',', header=None,\n",
    "                 names=['Airline','Airline ID','Source airport','Source airport ID',\n",
    "                        'Destination airport','Destination airport ID','Codeshare',\n",
    "                        'Stops','Equipment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates (only one trajectory per route)\n",
    "df = df[['Source airport','Destination airport']].drop_duplicates(keep='first', inplace=False)\n",
    "\n",
    "# Read the data into a dataframe (specifying the column names)\n",
    "df_airports = pd.read_csv('/Users/neil.watt/Documents/PythonScripts/Coronavirus/airports.dat.txt', sep=',', header=None,\n",
    "                          names=['Airport','Name','City','Country','IATA','ICAO',\n",
    "                                 'Latitude','Longitude','Altitude','Timezone','DST',\n",
    "                                 'Tz','Type','Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Airports and routes.html'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ALL LONDON AIRPORTS\n",
    "\n",
    "# Select only those routes starting or ending from a London Airport\n",
    "# in order London City, Heathrow, Gatwick, Luton, Stansted, Southend\n",
    "\n",
    "Country_List=UK_list_new\n",
    "\n",
    "#India_list_new\n",
    "\n",
    "# note pipemeans OR\n",
    "#df = df.loc[(df['Source airport'].isin(Iran_list_new)) | (df['Destination airport'].isin(India_list_new))]\n",
    "df = df.loc[(df['Source airport'].isin(Country_List)) | (df['Destination airport'].isin(Country_List))]\n",
    "\n",
    "#df = df.loc[(df['Source airport'].isin(['LCY','LHR','LGW','LTN','STN','SEN'])) | (df['Destination airport'].isin(['LCY','LHR','LGW','LTN','STN','SEN']))]\n",
    "\n",
    "# Append the origin airport's coordinates to the routes' dataframe\n",
    "df = pd.merge(df, df_airports[['IATA','Latitude','Longitude']],\n",
    "              how='inner', left_on='Source airport', right_on='IATA', suffixes=('_Orig','_Dest'))\n",
    "\n",
    "# Append the destination airport's coordinates to the routes' dataframe\n",
    "df = pd.merge(df, df_airports[['IATA','Latitude','Longitude']],\n",
    "              how='inner', left_on='Destination airport', right_on='IATA', suffixes=('_Orig','_Dest'))\n",
    "\n",
    "# Keep only Origin/Destination IATA ID columns, and their Latitude/Longitude\n",
    "df = df.drop(columns=['Source airport','Destination airport'])\n",
    "\n",
    "# Calculate the distance (great circle distance) between the origin and destination airports\n",
    "df['Distance'] = ''\n",
    "for index, row in df.iterrows():\n",
    "    df.loc[df.index==index,'Distance'] = dist(row.Latitude_Orig, row.Longitude_Orig, row.Latitude_Dest, row.Longitude_Dest)/1000\n",
    "\n",
    "# Initialise the data list that will be used to feed the plot\n",
    "data = []\n",
    "\n",
    "# Append all airports (blue dots) to the map\n",
    "data.append(dict(\n",
    "                type = 'scattergeo',\n",
    "                locationmode = 'ISO-3',\n",
    "                showlegend = False,\n",
    "                lon = df_airports['Longitude'],\n",
    "                lat = df_airports['Latitude'],\n",
    "                hoverinfo = 'text',\n",
    "                text = df_airports['IATA'],\n",
    "                mode = 'markers',\n",
    "                marker = dict(\n",
    "                    size=2,\n",
    "                    color='rgb(0, 0, 255)',\n",
    "                    line = dict(\n",
    "                        width=3,\n",
    "                        color='rgba(68, 68, 68, 0)'\n",
    "                    )\n",
    "                ))\n",
    "        )\n",
    "\n",
    "# Append the longest route to the map\n",
    "data.append(\n",
    "        dict(\n",
    "            type = 'scattergeo',\n",
    "            locationmode = 'ISO-3',\n",
    "            name = 'Longest Route',\n",
    "            showlegend = True,\n",
    "            lon = [ df.loc[df['Distance']==df['Distance'].max(),'Longitude_Orig'].values[0], df.loc[df['Distance']==df['Distance'].max(),'Longitude_Dest'].values[0] ],\n",
    "            lat = [ df.loc[df['Distance']==df['Distance'].max(),'Latitude_Orig'].values[0], df.loc[df['Distance']==df['Distance'].max(),'Latitude_Dest'].values[0] ],\n",
    "            mode = 'lines',\n",
    "            line = dict(\n",
    "                width = 2,\n",
    "                color = 'red',\n",
    "            ),\n",
    "            opacity = 1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "#India_list_new\n",
    "\n",
    "# Append all other routes to the map\n",
    "for i in range(len(df)):\n",
    "    data.append(\n",
    "        dict(\n",
    "            type = 'scattergeo',\n",
    "            locationmode = 'ISO-3',\n",
    "            name = str(df['IATA_Orig'][i]) + ' - ' + str(df['IATA_Dest'][i]),\n",
    "            #showlegend = True if df['IATA_Orig'][i] in India_list_new else False,\n",
    "            #showlegend = True if df['IATA_Orig'][i] in ['LCY','LHR','LGW','LTN','STN','SEN'] else False,\n",
    "            lon = [ df['Longitude_Orig'][i], df['Longitude_Dest'][i] ],\n",
    "            lat = [ df['Latitude_Orig'][i], df['Latitude_Dest'][i] ],\n",
    "            mode = 'lines',\n",
    "            line = dict(\n",
    "                width = 1,\n",
    "                color = 'green',\n",
    "            ),\n",
    "            opacity = 0.3,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Define the plot's layout\n",
    "layout = dict(\n",
    "        title = 'UK Airports and Routes',\n",
    "      \n",
    "    title_x=0.5,\n",
    "    showlegend = False,\n",
    "        geo = dict(\n",
    "            scope='world',\n",
    "            projection=dict( type='azimuthal equal area' ),\n",
    "            showland = True,\n",
    "            landcolor = 'rgb(255, 255, 255)',\n",
    "           # countrycolor = 'rgb(255, 255, 255)',\n",
    "            showcountries=True, countrycolor=\"Black\"\n",
    "            #showcountries = True,\n",
    "            \n",
    "            \n",
    "        ),\n",
    "    #title='Iran Airports and Routes',\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Create the figure to be plotted\n",
    "fig = dict( data=data, layout=layout )\n",
    "\n",
    "#py.plot(fig, world_readable=True)\n",
    "ol.plot(fig, filename='Airports and routes.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpl_toolkits.basemap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c3cd7dcf66e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#from mpl_toolkits import Basemap as Basemap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#from mpl_toolkits.basemap import Basemap as Basemap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mpl_toolkits.basemap'"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/catching-that-flight-visualizing-social-network-with-networkx-and-basemap-ce4a0d2eaea6\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap as Basemap\n",
    "#from mpl_toolkits import Basemap as Basemap\n",
    "#from mpl_toolkits.basemap import Basemap as Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'networkx' has no attribute 'from_pandas_dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-cd0447b75f31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pandas_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroutes_us\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Source Airport'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Dest Airport'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_attr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'number of flights'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcreate_using\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'networkx' has no attribute 'from_pandas_dataframe'"
     ]
    }
   ],
   "source": [
    "graph = nx.from_pandas_dataframe(routes_us, source = 'Source Airport', target = 'Dest Airport', edge_attr = 'number of flights',create_using = nx.DiGraph())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
